{"cells":[{"source":"# baseline代码：","cell_type":"markdown","metadata":{"trusted":true,"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"ADF20BC95932448A8F07A850D96B808A","mdEditEnable":false,"jupyter":{}}},{"metadata":{"id":"FC395C6D4F8F4DC18C1B495D776512FA","tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{}},"cell_type":"code","outputs":[],"source":"#! -*- coding:utf-8 -*-\n\nimport re\nimport gc\nimport codecs\nimport random\nimport warnings\nimport pandas as pd\nfrom sklearn.metrics import f1_score\nfrom nltk.metrics.distance import jaccard_distance\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras.optimizers import *\nfrom keras.initializers import *\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical\nfrom keras_bert import AdamWarmup, calc_train_steps\nfrom keras_bert import load_trained_model_from_checkpoint, Tokenizer\nfrom keras.backend.tensorflow_backend import set_session\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nset_session(tf.Session(config=config))\n\nmaxlen = 512\nconfig_path = 'F://workplace/BERT/chinese_roberta_wwm_ext_L-12_H-768_A-12/bert_config.json'\ncheckpoint_path = 'F://workplace/BERT/chinese_roberta_wwm_ext_L-12_H-768_A-12/bert_model.ckpt'\ndict_path = 'F://workplace/BERT/chinese_roberta_wwm_ext_L-12_H-768_A-12/vocab.txt'\n\nsub_dir = './result/bert_end2end_classfication_sub.csv'\ntrain_a_dir = './data/a/Train_Data.csv'\ntest_a_dir = './data/a/Test_Data.csv'\ntrain_b_dir = './data/b/round2_train.csv'\ntest_b_dir = './data/b/round2_test.csv'\n\n\ndef data_preprocess(x, s):\n    data = []\n    text_list = re.split(\"[。！!；;？]\", x[s])\n    entity_list = x['entity'].split(';')\n    for text in text_list:\n        for entity in entity_list:\n            if entity != '' and entity in text:\n                data.append(text)\n                break\n    if len(data) != 0:\n        data = '。'.join(data)\n    else:\n        data = x[s]\n    return data\n\n\nclass OurTokenizer(Tokenizer):\n    def _tokenize(self, text):\n        R = []\n        for c in text:\n            if c in self._token_dict:\n                R.append(c)\n            elif self._is_space(c):\n                R.append('[unused1]')  # space类用未经训练的[unused1]表示\n            else:\n                R.append('[UNK]')  # 剩余的字符是[UNK]\n        return R\n\n\ndef seq_padding(X, padding):\n    return np.array([np.concatenate([x, [padding] * (maxlen - len(x))]) if len(x) < maxlen else x[:maxlen] for x in X])\n\n\nclass data_generator:\n    def __init__(self, data, batch_size=8, shuffle=True):\n        self.data = data\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.steps = len(self.data) // self.batch_size\n        if len(self.data) % self.batch_size != 0:\n            self.steps += 1\n\n    def __len__(self):\n        return self.steps\n\n    def __iter__(self):\n        while True:\n            idxs = list(range(len(self.data)))\n\n            if self.shuffle:\n                np.random.shuffle(idxs)\n\n            X1, X2, Y = [], [], []\n            for i in idxs:\n                d = self.data[i]\n                x1, x2 = tokenizer.encode(first=d[0] + '_' + d[1] + '_' + d[2])\n                y = d[3]\n                X1.append(x1)\n                X2.append(x2)\n                Y.append(y)\n                if len(X1) == self.batch_size or i == idxs[-1]:\n                    X1 = seq_padding(X1, 0)\n                    X2 = seq_padding(X2, 0)\n                    Y = np.array(Y)\n                    yield [X1, X2], Y\n                    [X1, X2, Y] = [], [], []\n\n\ndef build_bert(nclass):\n    bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)\n\n    for l in bert_model.layers:\n        l.trainable = True\n\n    x1_in = Input(shape=(None,))\n    x2_in = Input(shape=(None,))\n    x = bert_model([x1_in, x2_in])\n    x = Lambda(lambda x: x[:, 0])(x)\n    x = Dense(nclass, activation='sigmoid')(x)\n\n    model = Model([x1_in, x2_in], x)\n    model.compile(loss='binary_crossentropy', optimizer=Adam(1e-5), metrics=['accuracy'])\n    print(model.summary())\n    return model\n\n\ndef run_cv(nfold, train_list, test_list):\n    train_preds = np.zeros((len(train_list), 1))\n    test_preds = np.zeros((len(test_list), 1))\n    skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=1017)\n    for i, (train_index, valid_index) in enumerate(skf.split(train_list, train_list[:, 3])):\n        print('第%s折开始训练' % (i + 1))\n\n        X_train, X_valid, = train_list[train_index, :], train_list[valid_index, :]\n        train_D = data_generator(X_train, batch_size=4, shuffle=True)\n        valid_D = data_generator(X_valid, batch_size=4, shuffle=True)\n        test_D = data_generator(test_list, batch_size=4, shuffle=False)\n\n        K.clear_session()\n        model = build_bert(1)\n        early_stopping = EarlyStopping(monitor='val_loss', patience=1, verbose=1, mode='min')\n        plateau = ReduceLROnPlateau(monitor=\"val_loss\", verbose=1, mode='min', factor=0.5, patience=2)\n        checkpoint = ModelCheckpoint('./model/bert_end2end_classfication_fold_' + str(i + 1) + '.h5', monitor='val_loss',\n                                     verbose=2, save_best_only=True, mode='min', save_weights_only=True)\n        model.fit_generator(\n            train_D.__iter__(),\n            steps_per_epoch=len(train_D),\n            epochs=10,\n            validation_data=valid_D.__iter__(),\n            validation_steps=len(valid_D),\n            callbacks=[early_stopping, checkpoint],\n        )\n        model.save('./model/bert_end2end_classfication_fold_' + str(i + 1) + '.h5')\n        train_preds[valid_index] = model.predict_generator(valid_D.__iter__(), steps=len(valid_D), verbose=1)\n        test_preds += model.predict_generator(test_D.__iter__(), steps=len(test_D), verbose=1) / nfold\n\n        del model\n        gc.collect()\n\n    return train_preds, test_preds\n\ntrain_a = pd.read_csv(train_a_dir, index_col=None)\ntrain_b = pd.read_csv(train_b_dir, index_col=None)\ntrain = pd.concat([train_a, train_b], axis=0, ignore_index=True)\ntest = pd.read_csv(test_b_dir, index_col=None)\n\nprint('train', train.shape)\nprint('test', test.shape)\nprint(train.columns)\nprint(test.columns)\n\nprint('train content')\ntrain.fillna('', inplace=True)\ntrain['title'] = train.apply(lambda x: data_preprocess(x, 'title'), axis=1)\ntrain['text'] = train.apply(lambda x: data_preprocess(x, 'text'), axis=1)\ntrain['content'] = train.apply(lambda x: x['title'] + '_' + x['text'], axis=1)\n\nprint('test content')\ntest.fillna('', inplace=True)\ntest['title'] = test.apply(lambda x: data_preprocess(x, 'title'), axis=1)\ntest['text'] = test.apply(lambda x: data_preprocess(x, 'text'), axis=1)\ntest['content'] = test.apply(lambda x: x['title'] + '_' + x['text'], axis=1)\n\nprint('token_dict')\ntoken_dict = {}\nwith codecs.open(dict_path, 'r', 'utf8') as reader:\n    for line in reader:\n        token = line.strip()\n        token_dict[token] = len(token_dict)\n\nprint('tokenizer')\ntokenizer = OurTokenizer(token_dict)\n\nprint('train_list')\ntrain_id = []\ntrain_list = []\nfor i in train.index:\n    data_row = train.iloc[i]\n    entitys = data_row['entity'].split(';')\n    for entity in entitys:\n        if len(entity) != 0:\n            train_id.append(data_row['id'])\n            if entity in data_row['key_entity'].split(';'):\n                train_list.append((entity, data_row['entity'], data_row['content'], 1))\n            else:\n                train_list.append((entity, data_row['entity'], data_row['content'], 0))\ntrain_list = np.array(train_list)\n\nprint('test_list')\ntest_id = []\ntest_list = []\nfor i in test.index:\n    data_row = test.iloc[i]\n    entitys = data_row['entity'].split(';')\n    for entity in entitys:\n        if len(entity) != 0:\n            test_id.append(data_row['id'])\n            test_list.append((entity, data_row['entity'], data_row['content'], 0))\ntest_list = np.array(test_list)\n\nprint('start train')\ntrain_preds, test_preds = run_cv(5, train_list, test_list)\n\ntrain_preds = train_preds[:, 0]\ntest_preds = test_preds[:, 0]\ntest_pre_result = [1 if test_pred > 0.5 else 0 for test_pred in test_preds]\n\nid_list = []\nnegative_list = []\nkey_entity_list = []\ntmp = pd.DataFrame({'id': test_id, 'entity': test_list[:, 0], 'pre_result': test_pre_result})\nfor id in tmp['id'].unique():\n    df = tmp[(tmp['id'] == id) & (tmp['pre_result'] == 1)]\n    entitys = df['entity'].tolist()\n    if len(entitys) > 0:\n        id_list.append(id)\n        negative_list.append(1)\n        key_entity_list.append(';'.join(entitys))\n    else:\n        id_list.append(id)\n        negative_list.append(0)\n        key_entity_list.append('')\n\nsub = pd.DataFrame({'id': id_list, 'negative': negative_list, 'key_entity': key_entity_list})\nsub = pd.merge(test[['id']], sub, on='id', how='left')\nsub['negative'] = sub['negative'].fillna(0).astype(int)\nsub['key_entity'] = sub['key_entity'].fillna('')\nprint(sub['negative'].value_counts())\nsub.to_csv(sub_dir, index=None)","execution_count":null},{"metadata":{"id":"9AF73C9DD9FB4CA891DEFBA209B17B3D","tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false,"jupyter":{}},"cell_type":"markdown","source":"# **参考：**\n\n比赛：https://www.datafountain.cn/competitions/353\n\n冠军：https://github.com/xiong666/ccf_financial_negative\n\n线上第一名：https://github.com/A-Rain/BDCI2019-Negative_Finance_Info_Judge\n\n第二名：https://github.com/rebornZH/2019-CCF-BDCI-NLP\n\n第三名：https://github.com/Chevalier1024/CCF-BDCI-ABSA"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}